{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import math\n",
    "import psutil\n",
    "import time\n",
    "import datetime\n",
    "from io import open\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.cuda\n",
    "\n",
    "\"\"\"this line clears sys to allow for argparse to work as gradient clipper\"\"\"\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function converts a Unicode string to plain ASCII \n",
    "from https://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "def uniToAscii(sentence):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', sentence)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"Lowercase, trim, and remove non-letter characters (from pytorch)\"\"\"\n",
    "def normalizeString(s):\n",
    "    s = re.sub(r\" ##AT##-##AT## \", r\" \", s)\n",
    "    s = uniToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s   \n",
    "\n",
    "  \n",
    "\"\"\"Denote patterns that sentences must start with to be kept in dataset. \n",
    "Can be changed if desired (from pytorch)\"\"\"\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Filters each input-output pair, keeping sentences that are less than max_length \n",
    "if start_filter is true, also filters out sentences that don't start with eng_prefixes\"\"\"\n",
    "def filterPair(p, max_length, start_filter):\n",
    "    filtered = len(p[0].split(' ')) < max_length and \\\n",
    "        len(p[1].split(' ')) < max_length \n",
    "    if start_filter:\n",
    "        return filtered and p[1].startswith(eng_prefixes)\n",
    "    else:\n",
    "        return filtered\n",
    "\n",
    "\"\"\"Filters all of the input-output language pairs in the dataset using filterPair \n",
    "for each pair (from pytorch)\"\"\"\n",
    "def filterPairs(pairs, max_length, start_filter):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length, start_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start of sentence tag\"\"\"\n",
    "SOS_token = 0\n",
    "\n",
    "\"\"\"end of sentence tag\"\"\"\n",
    "EOS_token = 1\n",
    "\n",
    "\"\"\"unknown word tag (this is used to handle words that are not in our Vocabulary)\"\"\"\n",
    "UNK_token = 2\n",
    "\n",
    "\n",
    "\"\"\"Lang class, used to store the vocabulary of each language\"\"\"\n",
    "class Lang:\n",
    "    def __init__(self, language):\n",
    "        self.language_name = language\n",
    "        self.word_to_index = {\"SOS\":SOS_token, \"EOS\":EOS_token, \"<UNK>\":UNK_token}\n",
    "        self.word_to_count = {}\n",
    "        self.index_to_word = {SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<UNK>\"}\n",
    "        self.vocab_size = 3\n",
    "        self.cutoff_point = -1\n",
    "\n",
    "\n",
    "    def countSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.countWords(word)\n",
    "\n",
    "    \"\"\"counts the number of times each word appears in the dataset\"\"\"\n",
    "    def countWords(self, word):\n",
    "        if word not in self.word_to_count:\n",
    "            self.word_to_count[word] = 1\n",
    "        else:\n",
    "            self.word_to_count[word] += 1\n",
    "\n",
    "    \"\"\"if the number of unique words in the dataset is larger than the\n",
    "    specified max_vocab_size, creates a cutoff point that is used to\n",
    "    leave infrequent words out of the vocabulary\"\"\"\n",
    "    def createCutoff(self, max_vocab_size):\n",
    "        word_freqs = list(self.word_to_count.values())\n",
    "        word_freqs.sort(reverse=True)\n",
    "        if len(word_freqs) > max_vocab_size:\n",
    "            self.cutoff_point = word_freqs[max_vocab_size]\n",
    "\n",
    "    \"\"\"assigns each unique word in a sentence a unique index\"\"\"\n",
    "    def addSentence(self, sentence):\n",
    "        new_sentence = ''\n",
    "        for word in sentence.split(' '):\n",
    "            unk_word = self.addWord(word)\n",
    "            if not new_sentence:\n",
    "                new_sentence =unk_word\n",
    "            else:\n",
    "                new_sentence = new_sentence + ' ' + unk_word\n",
    "        return new_sentence\n",
    "\n",
    "    \"\"\"assigns a word a unique index if not already in vocabulary\n",
    "    and it appeaars often enough in the dataset\n",
    "    (self.word_to_count is larger than self.cutoff_point)\"\"\"\n",
    "    def addWord(self, word):\n",
    "        if self.word_to_count[word] > self.cutoff_point:\n",
    "            if word not in self.word_to_index:\n",
    "                self.word_to_index[word] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "            return word\n",
    "        else:\n",
    "            return self.index_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prepares both the input and output Lang classes from the passed dataset'''\n",
    "\n",
    "def prepareLangs(lang1, lang2, file_path, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    if len(file_path) == 2:\n",
    "        lang1_lines = open(file_path[0], encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "\n",
    "        lang2_lines = open(file_path[1], encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "\n",
    "        if len(lang1_lines) != len(lang2_lines):\n",
    "            print(\"Input and output text sizes do not align\")\n",
    "            print(\"Number of lang1 lines: %s \" %len(lang1_lines))\n",
    "            print(\"Number of lang2 lines: %s \" %len(lang2_lines))\n",
    "            quit()\n",
    "\n",
    "        pairs = []\n",
    "\n",
    "        for line in range(len(lang1_lines)):\n",
    "            pairs.append([normalizeString(lang1_lines[line]),\n",
    "                          normalizeString(lang2_lines[line])])            \n",
    "\n",
    "\n",
    "    elif len(file_path) == 1:\n",
    "        lines = open(file_path[0], encoding='utf-8').\\\n",
    "    \tread().strip().split('\\n')\n",
    "        pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "    '''prepares both the input and output Lang classes from the passed dataset'''\n",
    "\n",
    "def prepareLangs(lang1, lang2, file_path, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    if len(file_path) == 2:\n",
    "        lang1_lines = open(file_path[0], encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "\n",
    "        lang2_lines = open(file_path[1], encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "\n",
    "        if len(lang1_lines) != len(lang2_lines):\n",
    "            print(\"Input and output text sizes do not align\")\n",
    "            print(\"Number of lang1 lines: %s \" %len(lang1_lines))\n",
    "            print(\"Number of lang2 lines: %s \" %len(lang2_lines))\n",
    "            quit()\n",
    "\n",
    "        pairs = []\n",
    "\n",
    "        for line in range(len(lang1_lines)):\n",
    "            pairs.append([normalizeString(lang1_lines[line]),\n",
    "                          normalizeString(lang2_lines[line])])            \n",
    "\n",
    "\n",
    "    elif len(file_path) == 1:\n",
    "        lines = open(file_path[0], encoding='utf-8').\\\n",
    "    \tread().strip().split('\\n')\n",
    "        pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"completely prepares both input and output languages \n",
    "and returns cleaned and trimmed train and test pairs\"\"\"\n",
    "\n",
    "def prepareData(lang1, lang2, file_path, max_vocab_size=50000, \n",
    "                reverse=False, trim=0, start_filter=False, perc_train_set=0.9, \n",
    "                print_to=None):\n",
    "    \n",
    "    input_lang, output_lang, pairs = prepareLangs(lang1, lang2, \n",
    "                                                  file_path, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    if print_to:\n",
    "        with open(print_to,'a') as f:\n",
    "            f.write(\"Read %s sentence pairs \\n\" % len(pairs))\n",
    "    \n",
    "    if trim != 0:\n",
    "        pairs = filterPairs(pairs, trim, start_filter)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        if print_to:\n",
    "            with open(print_to,'a') as f:\n",
    "                f.write(\"Read %s sentence pairs \\n\" % len(pairs))\n",
    "\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.countSentence(pair[0])\n",
    "        output_lang.countSentence(pair[1])\n",
    "\n",
    "\n",
    "    input_lang.createCutoff(max_vocab_size)\n",
    "    output_lang.createCutoff(max_vocab_size)\n",
    "\n",
    "    pairs = [(input_lang.addSentence(pair[0]),output_lang.addSentence(pair[1])) \n",
    "             for pair in pairs]\n",
    "\n",
    "    shuffle(pairs)\n",
    "    \n",
    "    train_pairs = pairs[:math.ceil(perc_train_set*len(pairs))]\n",
    "    test_pairs = pairs[math.ceil(perc_train_set*len(pairs)):]\n",
    "\n",
    "    print(\"Train pairs: %s\" % (len(train_pairs)))\n",
    "    print(\"Test pairs: %s\" % (len(test_pairs)))\n",
    "    print(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
    "    print(\"%s, %s -> %s\" % (input_lang.language_name, len(input_lang.word_to_count),\n",
    "                            input_lang.vocab_size,))\n",
    "    print(\"%s, %s -> %s\" % (output_lang.language_name, len(output_lang.word_to_count), \n",
    "                            output_lang.vocab_size))\n",
    "    print()\n",
    "\n",
    "    if print_to:\n",
    "        with open(print_to,'a') as f:\n",
    "            f.write(\"Train pairs: %s\" % (len(train_pairs)))\n",
    "            f.write(\"Test pairs: %s\" % (len(test_pairs)))\n",
    "            f.write(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
    "            f.write(\"%s, %s -> %s\" % (input_lang.language_name, \n",
    "                                      len(input_lang.word_to_count),\n",
    "                                      input_lang.vocab_size,))\n",
    "            f.write(\"%s, %s -> %s \\n\" % (output_lang.language_name, len(output_lang.word_to_count), \n",
    "                            output_lang.vocab_size))\n",
    "        \n",
    "    return input_lang, output_lang, train_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"converts a sentence to one hot encoding vectors - pytorch allows us to just\n",
    "use the number corresponding to the unique index for that word,\n",
    "rather than a complete one hot encoding vector for each word\"\"\"\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    indexes = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            indexes.append(lang.word_to_index[word])\n",
    "        except:\n",
    "            indexes.append(lang.word_to_index[\"<UNK>\"])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = torch.LongTensor(indexes).view(-1)\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "      \n",
    "\"\"\"converts a pair of sentence (input and target) to a pair of tensors\"\"\"\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_variable = tensorFromSentence(input_lang, pair[0])\n",
    "    target_variable = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "  \n",
    "\n",
    "\"\"\"converts from tensor of one hot encoding vector indices to sentence\"\"\"\n",
    "def sentenceFromTensor(lang, tensor):\n",
    "    raw = tensor.data\n",
    "    words = []\n",
    "    for num in raw:\n",
    "        words.append(lang.index_to_word[num.item()])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"seperates data into batches of size batch_size\"\"\"\n",
    "def batchify(data, input_lang, output_lang, batch_size, shuffle_data=True):\n",
    "    if shuffle_data == True:\n",
    "        shuffle(data)\n",
    "    number_of_batches = len(data) // batch_size\n",
    "    batches = list(range(number_of_batches))\n",
    "    longest_elements = list(range(number_of_batches))\n",
    "    \n",
    "    for batch_number in range(number_of_batches):\n",
    "        longest_input = 0\n",
    "        longest_target = 0\n",
    "        input_variables = list(range(batch_size))\n",
    "        target_variables = list(range(batch_size))\n",
    "        index = 0      \n",
    "        for pair in range((batch_number*batch_size),((batch_number+1)*batch_size)):\n",
    "            input_variables[index], target_variables[index] = tensorsFromPair(input_lang, output_lang, data[pair])\n",
    "            if len(input_variables[index]) >= longest_input:\n",
    "                longest_input = len(input_variables[index])\n",
    "            if len(target_variables[index]) >= longest_target:\n",
    "                longest_target = len(target_variables[index])\n",
    "            index += 1\n",
    "        batches[batch_number] = (input_variables, target_variables)\n",
    "        longest_elements[batch_number] = (longest_input, longest_target)\n",
    "    return batches , longest_elements, number_of_batches\n",
    "\n",
    "\n",
    "\"\"\"pads batches to allow for sentences of variable lengths to be computed in parallel\"\"\"\n",
    "def pad_batch(batch):\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(batch[0],padding_value=EOS_token)\n",
    "    padded_targets = torch.nn.utils.rnn.pad_sequence(batch[1],padding_value=EOS_token)\n",
    "    return (padded_inputs, padded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "\tdef __init__(self,input_size,hidden_size,layers=1,dropout=0.1,\n",
    "               bidirectional=True):\n",
    "\t\tsuper(EncoderRNN, self).__init__()\n",
    "\n",
    "\t\tif bidirectional:\n",
    "\t\t\tself.directions = 2\n",
    "\t\telse:\n",
    "\t\t\tself.directions = 1\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.embedder = nn.Embedding(input_size,hidden_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                        num_layers=layers,dropout=dropout,\n",
    "                        bidirectional=bidirectional,batch_first=False)\n",
    "\t\tself.fc = nn.Linear(hidden_size*self.directions, hidden_size)\n",
    "\n",
    "\tdef forward(self, input_data, h_hidden, c_hidden):\n",
    "\t\tembedded_data = self.embedder(input_data)\n",
    "\t\tembedded_data = self.dropout(embedded_data)\n",
    "\t\thiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\n",
    "\n",
    "\t\treturn hiddens, outputs\n",
    "\n",
    "\t\"\"\"creates initial hidden states for encoder corresponding to batch size\"\"\"\n",
    "\tdef create_init_hiddens(self, batch_size):\n",
    "\t\th_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
    "                                    batch_size, self.hidden_size))\n",
    "\t\tc_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
    "                                    batch_size, self.hidden_size))\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\treturn h_hidden.cuda(), c_hidden.cuda()\n",
    "\t\telse:\n",
    "\t\t\treturn h_hidden, c_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttn(nn.Module):\n",
    "\tdef __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
    "\t\tsuper(DecoderAttn, self).__init__()\n",
    "\n",
    "\t\tif bidirectional:\n",
    "\t\t\tself.directions = 2\n",
    "\t\telse:\n",
    "\t\t\tself.directions = 1\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.embedder = nn.Embedding(output_size,hidden_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.score_learner = nn.Linear(hidden_size*self.directions, \n",
    "                                   hidden_size*self.directions)\n",
    "\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                        num_layers=layers,dropout=dropout,\n",
    "                        bidirectional=bidirectional,batch_first=False)\n",
    "\t\tself.context_combiner = nn.Linear((hidden_size*self.directions)\n",
    "                                      +(hidden_size*self.directions), hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.output = nn.Linear(hidden_size, output_size)\n",
    "\t\tself.soft = nn.Softmax(dim=1)\n",
    "\t\tself.log_soft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "\tdef forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
    "\n",
    "\t\tembedded_data = self.embedder(input_data)\n",
    "\t\tembedded_data = self.dropout(embedded_data)\t\n",
    "\t\tbatch_size = embedded_data.shape[1]\n",
    "\t\thiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\t\n",
    "\t\ttop_hidden = outputs[0].view(self.num_layers,self.directions,\n",
    "                                 hiddens.shape[1],\n",
    "                                 self.hidden_size)[self.num_layers-1]\n",
    "\t\ttop_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1, 1)\n",
    "\n",
    "\t\tprep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n",
    "\t\tscores = torch.bmm(prep_scores, top_hidden)\n",
    "\t\tattn_scores = self.soft(scores)\n",
    "\t\tcon_mat = torch.bmm(encoder_hiddens.permute(1,2,0),attn_scores)\n",
    "\t\th_tilde = self.tanh(self.context_combiner(torch.cat((con_mat,\n",
    "                                                         top_hidden),dim=1)\n",
    "                                              .view(batch_size,-1)))\n",
    "\t\tpred = self.output(h_tilde)\n",
    "\t\tpred = self.log_soft(pred)\n",
    "\n",
    "\t\t\n",
    "\t\treturn pred, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Performs training on a single batch of training data. Computing the loss \n",
    "according to the passed loss_criterion and back-propagating on this loss.'''\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, \n",
    "                encoder_optimizer, decoder_optimizer, loss_criterion):\n",
    "\tencoder_optimizer.zero_grad()\n",
    "\tdecoder_optimizer.zero_grad()\n",
    "\tloss = 0\n",
    "\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
    "\n",
    "\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                           fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\telse Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                        fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\n",
    "\tdec_h_hidden = enc_outputs[0]\n",
    "\tdec_c_hidden = enc_outputs[1]\n",
    "\t\n",
    "\tfor i in range(target_batch.shape[0]):\n",
    "\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, \n",
    "                                dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\tdecoder_input = target_batch[i].view(1,-1)\n",
    "\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\tdec_c_hidden = dec_outputs[1]\n",
    "\t\t\n",
    "\t\tloss += loss_criterion(pred,target_batch[i])\n",
    "\n",
    "\n",
    "\tloss.backward()\n",
    "\n",
    "\ttorch.nn.utils.clip_grad_norm_(encoder.parameters(),args.clip)\n",
    "\ttorch.nn.utils.clip_grad_norm_(decoder.parameters(),args.clip)\n",
    "\n",
    "\tencoder_optimizer.step()\n",
    "\tdecoder_optimizer.step()\n",
    "\n",
    "\treturn loss.item() / target_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Performs a complete epoch of training through all of the training_batches'''\n",
    "\n",
    "def train(train_batches, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion):\n",
    "\n",
    "\tround_loss = 0\n",
    "\ti = 1\n",
    "\tfor batch in train_batches:\n",
    "\t\ti += 1\n",
    "\t\t(input_batch, target_batch) = pad_batch(batch)\n",
    "\t\tbatch_loss = train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion)\n",
    "\t\tround_loss += batch_loss\n",
    "\n",
    "\treturn round_loss / len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluates the loss on a single batch of test data. Computing the loss \n",
    "according to the passed loss_criterion. Does not perform back-prop'''\n",
    "\n",
    "def test_batch(input_batch, target_batch, encoder, decoder, loss_criterion):\n",
    "\t\n",
    "\tloss = 0\n",
    "\n",
    "\t#create initial hidde state for encoder\n",
    "\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
    "\n",
    "\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                           fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\telse Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                        fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\tdec_h_hidden = enc_outputs[0]\n",
    "\tdec_c_hidden = enc_outputs[1]\n",
    "\t\n",
    "\tfor i in range(target_batch.shape[0]):\n",
    "\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\ttopv, topi = pred.topk(1,dim=1)\n",
    "\t\tni = topi.view(1,-1)\n",
    "\t\t\n",
    "\t\tdecoder_input = ni\n",
    "\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\tdec_c_hidden = dec_outputs[1]\n",
    "\n",
    "\t\tloss += loss_criterion(pred,target_batch[i])\n",
    "\t\t\n",
    "\treturn loss.item() / target_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Computes the loss value over all of the test_batches'''\n",
    "\n",
    "def test(test_batches, encoder, decoder, loss_criterion):\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss = 0\n",
    "\n",
    "\t\tfor batch in test_batches:\n",
    "\t\t\t(input_batch, target_batch) = pad_batch(batch)\n",
    "\t\t\tbatch_loss = test_batch(input_batch, target_batch, encoder, decoder, loss_criterion)\n",
    "\t\t\ttest_loss += batch_loss\n",
    "\n",
    "\treturn test_loss / len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Returns the predicted translation of a given input sentence. Predicted\n",
    "translation is trimmed to length of cutoff_length argument'''\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, cutoff_length):\n",
    "\twith torch.no_grad():\n",
    "\t\tinput_variable = tensorFromSentence(input_lang, sentence)\n",
    "\t\tinput_variable = input_variable.view(-1,1)\n",
    "\t\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(1)\n",
    "\n",
    "\t\tenc_hiddens, enc_outputs = encoder(input_variable, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\t\tdec_h_hidden = enc_outputs[0]\n",
    "\t\tdec_c_hidden = enc_outputs[1]\n",
    "\n",
    "\t\tdecoded_words = []\n",
    "\n",
    "\t\tfor di in range(cutoff_length):\n",
    "\t\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\t\ttopv, topi = pred.topk(1,dim=1)\n",
    "\t\t\tni = topi.item()\n",
    "\t\t\tif ni == output_lang.word_to_index.get(\"EOS\"):\n",
    "\t\t\t\tdecoded_words.append('<EOS>')\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_words.append(output_lang.index_to_word[ni])\n",
    "\n",
    "\t\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(ni).cuda()) if use_cuda \\\n",
    "\t\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(ni))\n",
    "\t\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\t\tdec_c_hidden = dec_outputs[1]\n",
    "\n",
    "\t\toutput_sentence = ' '.join(decoded_words)\n",
    "    \n",
    "\t\treturn output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluates prediction translations for a specified number (n) of sentences\n",
    "chosen randomly from a list of passed sentence pairs. Returns three sentences\n",
    "in the format:\n",
    "                  > input sentence\n",
    "                  = correct translation\n",
    "                  < predicted translation'''\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, pairs, n=2, trim=100):\n",
    "\tfor i in range(n):\n",
    "\t\tpair = random.choice(pairs)\n",
    "\t\tprint('>', pair[0])\n",
    "\t\tprint('=', pair[1])\n",
    "\t\toutput_sentence = evaluate(encoder, decoder, pair[0],cutoff_length=trim)\n",
    "\t\tprint('<', output_sentence)\n",
    "\t\tprint('')    \n",
    "\t\tif create_txt:\n",
    "\t\t\tf = open(print_to, 'a')\n",
    "\t\t\tf.write(\"\\n \\\n",
    "\t\t\t\t> %s \\n \\\n",
    "\t\t\t\t= %s \\n \\\n",
    "\t\t\t\t< %s \\n\" % (pair[0], pair[1], output_sentence))\n",
    "\t\t\tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Used to plot the progress of training. Plots the loss value vs. time'''\n",
    "def showPlot(times, losses, fig_name):\n",
    "    x_axis_label = 'Minutes'\n",
    "    colors = ('red','blue')\n",
    "    if max(times) >= 120:\n",
    "    \ttimes = [mins/60 for mins in times]\n",
    "    \tx_axis_label = 'Hours'\n",
    "    i = 0\n",
    "    for key, losses in losses.items():\n",
    "    \tif len(losses) > 0:\n",
    "    \t\tplt.plot(times, losses, label=key, color=colors[i])\n",
    "    \t\ti += 1\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(x_axis_label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Results')\n",
    "    plt.savefig(fig_name+'.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "'''prints the current memory consumption'''\n",
    "def mem():\n",
    "\tif use_cuda:\n",
    "\t\tmem = torch.cuda.memory_allocated()/1e7\n",
    "\telse:\n",
    "\t\tmem = psutil.cpu_percent()\n",
    "\tprint('Current mem usage:')\n",
    "\tprint(mem)\n",
    "\treturn \"Current mem usage: %s \\n\" % (mem)\n",
    "\n",
    "'''converts a time measurement in seconds to hours'''\n",
    "def asHours(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\th = math.floor(m / 60)\n",
    "\ts -= m * 60\n",
    "\tm -= h * 60\n",
    "\treturn '%dh %dm %ds' % (h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The master function that trains the model. Evlautes progress on the train set\n",
    "(if present) and also records the progress of training in both a txt file and\n",
    "a png graph. Also can save the weights of both the Encoder and Decoder \n",
    "for future use.'''\n",
    "\n",
    "def train_and_test(epochs, test_eval_every, plot_every, learning_rate, \n",
    "                   lr_schedule, train_pairs, test_pairs, input_lang, \n",
    "                   output_lang, batch_size, test_batch_size, encoder, decoder, \n",
    "                   loss_criterion, trim, save_weights):\n",
    "\t\n",
    "\ttimes = []\n",
    "\tlosses = {'train set':[], 'test set': []}\n",
    "\n",
    "\ttest_batches, longest_seq, n_o_b = batchify(test_pairs, input_lang, \n",
    "                                              output_lang, test_batch_size, \n",
    "                                              shuffle_data=False)\n",
    "\n",
    "\tstart = time.time()\n",
    "\tfor i in range(1,epochs+1):\n",
    "    \n",
    "\t\t'''adjust the learning rate according to the learning rate schedule\n",
    "\t\tspecified in lr_schedule'''\n",
    "\t\tif i in lr_schedule.keys():\n",
    "\t\t\tlearning_rate /= lr_schedule.get(i)\n",
    "\n",
    "\n",
    "\t\tencoder.train()\n",
    "\t\tdecoder.train()\n",
    "\n",
    "\t\tencoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "\t\tdecoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "\t\tbatches, longest_seq, n_o_b = batchify(train_pairs, input_lang, \n",
    "                                           output_lang, batch_size, \n",
    "                                           shuffle_data=True)\n",
    "\t\ttrain_loss = train(batches, encoder, decoder, encoder_optimizer, \n",
    "                       decoder_optimizer, loss_criterion)\n",
    "\t\t\n",
    "\t\tnow = time.time()\n",
    "\t\tprint(\"Iter: %s \\nLearning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \n",
    "          % (i, learning_rate, asHours(now-start), train_loss))\n",
    "\n",
    "\t\tif create_txt:\n",
    "\t\t\twith open(print_to, 'a') as f:\n",
    "\t\t\t\tf.write(\"Iter: %s \\nLeaning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \\\n",
    "\t\t\t\t\t% (i, learning_rate, asHours(now-start), train_loss))\n",
    "\n",
    "\t\tif i % test_eval_every == 0:\n",
    "\t\t\tif test_pairs:\n",
    "\t\t\t\ttest_loss = test(test_batches, encoder, decoder, criterion)\n",
    "\t\t\t\tprint(\"Test set loss: %s\" % (test_loss))\n",
    "\t\t\t\tif create_txt:\n",
    "\t\t\t\t\twith open(print_to, 'a') as f:\n",
    "\t\t\t\t\t\tf.write(\"Test Loss: %s \\n\" % (test_loss))\n",
    "\t\t\t\tevaluate_randomly(encoder, decoder, test_pairs, trim)\n",
    "\t\t\telse:\n",
    "\t\t\t\tevaluate_randomly(encoder, decoder, train_pairs, trim)\n",
    "\n",
    "\t\tif i % plot_every == 0:\n",
    "\t\t\ttimes.append((time.time()-start)/60)\n",
    "\t\t\tlosses['train set'].append(train_loss)\n",
    "\t\t\tif test_pairs:\n",
    "\t\t\t\tlosses['test set'].append(test_loss)\n",
    "\t\t\tshowPlot(times, losses, output_file_name)\n",
    "\t\t\tif save_weights:\n",
    "\t\t\t\ttorch.save(encoder.state_dict(), output_file_name+'_enc_weights.pt')\n",
    "\t\t\t\ttorch.save(decoder.state_dict(), output_file_name+'_dec_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1\n",
       "0  Go.      Ve.\n",
       "1  Go.    Vete.\n",
       "2  Go.    Vaya.\n",
       "3  Go.  Váyase.\n",
       "4  Hi.    Hola."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded = pd.read_csv('./spa-eng/spa.txt', delimiter='\\t', header=None)\n",
    "uploaded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PROVIDE INFORMATION ON THE DATASET AND DATASET CLEANING\"\"\"\n",
    "\n",
    "input_lang_name = 'en' #input_lang_name = 'fre'\n",
    "output_lang_name = 'spa' #output_lang_name = 'en'\n",
    "\n",
    "\"\"\"name of your dataset\"\"\"\n",
    "dataset = 'orig'\n",
    "\n",
    "\"\"\"file path of dataset in the form of a tuple. If translated sentences are\n",
    "stored in two files, this tuple will have two elements\"\"\"\n",
    "raw_data_file_path = ('./spa-eng/spa.txt',) #raw_data_file_path = ('eng-fra.txt',)\n",
    "\n",
    "\"\"\"True if you want to reverse the order of the sentence pairs. For example, \n",
    "in our dataset the sentence pairs list the English sentence first followed by\n",
    "the French translation. But we want to translate from French to English,\n",
    "so we set reverse as True.\"\"\"\n",
    "reverse=False #reverse=True\n",
    "\n",
    "\"\"\"Remove sentences from dataset that are longer than trim (in either language)\"\"\"\n",
    "trim = 10 #trim = 10\n",
    "\n",
    "\"\"\"max number of words in the vocabulary for both languages\"\"\"\n",
    "max_vocab_size= 25000 #max_vocab_size= 20000\n",
    "\n",
    "\"\"\"if true removes sentences from the dataset that don't start with eng_prefixes.\n",
    "Typically will want to use False, but implemented to compare results with Pytorch\n",
    "tutorial. Can also change the eng_prefixes to prefixes of other languages or\n",
    "other English prefixes. Just be sure that the prefixes apply to the OUTPUT\n",
    "language (i.e. the language that the model is translating to NOT from)\"\"\"\n",
    "start_filter = False #start_filter = True\n",
    "\n",
    "\"\"\"denotes what percentage of the data to use as training data. the remaining \n",
    "percentage becomes test data. Typically want to use 0.8-0.9. 1.0 used here to \n",
    "compare with PyTorch results where no test set was utilized\"\"\"\n",
    "perc_train_set = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"denotes how often to evaluate a loss on the test set and print\n",
    "sample predictions on the test set.\n",
    "if no test set, simply prints sample predictions on the train set.\"\"\"\n",
    "test_eval_every = 1\n",
    "\n",
    "\"\"\"denotes how often to plot the loss values of train and test (if applicable)\"\"\"\n",
    "plot_every = 1\n",
    "\n",
    "\"\"\"if true creates a txt file of the output\"\"\"\n",
    "create_txt = True #False\n",
    "\n",
    "\"\"\"if true saves the encoder and decoder weights to seperate .pt files for later use\"\"\"\n",
    "save_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HYPERPARAMETERS: FEEL FREE TO PLAY WITH THESE TO TRY TO ACHIEVE BETTER RESULTS\"\"\"\n",
    "\n",
    "\"\"\"signifies whether the Encoder and Decoder should be bidirectional LSTMs or not\"\"\"\n",
    "bidirectional = True\n",
    "if bidirectional:\n",
    "\tdirections = 2\n",
    "else:\n",
    "\tdirections = 1\n",
    "\n",
    "\"\"\"number of layers in both the Encoder and Decoder\"\"\"\n",
    "layers = 2\n",
    "\n",
    "\"\"\"Hidden size of the Encoder and Decoder\"\"\"\n",
    "hidden_size = 440\n",
    "\n",
    "\"\"\"Dropout value for Encoder and Decoder\"\"\"\n",
    "dropout = 0.2\n",
    "\n",
    "\"\"\"Training set batch size\"\"\"\n",
    "batch_size = 32\n",
    "\n",
    "\"\"\"Test set batch size\"\"\"\n",
    "test_batch_size = 32\n",
    "\n",
    "\"\"\"number of epochs (full passes through the training data)\"\"\"\n",
    "epochs = 100\n",
    "\n",
    "\"\"\"Initial learning rate\"\"\"\n",
    "learning_rate= 1\n",
    "\n",
    "\"\"\"Learning rate schedule. Signifies by what factor to divide the learning rate\n",
    "at a certain epoch. For example {5:10} would divide the learning rate by 10\n",
    "before the 5th epoch and {5:10, 10:100} would divide the learning rate by 10\n",
    "before the 5th epoch and then again by 100 before the 10th epoch\"\"\"\n",
    "lr_schedule = {}\n",
    "\n",
    "\"\"\"loss criterion, see https://pytorch.org/docs/stable/nn.html for other options\"\"\"\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 118964 sentence pairs\n",
      "Trimmed to 90751 sentence pairs\n",
      "Counting words...\n",
      "Train pairs: 72601\n",
      "Test pairs: 18150\n",
      "Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\n",
      "en, 10761 -> 10764\n",
      "spa, 20219 -> 20222\n",
      "\n",
      "Train Pairs #\n",
      "72601\n",
      "Current mem usage:\n",
      "0.0\n",
      "Current mem usage:\n",
      "0.0\n",
      "Encoder and Decoder Created\n",
      "Current mem usage:\n",
      "0.0\n",
      "Cuda being used\n",
      "Number of epochs: 100\n",
      "Current mem usage:\n",
      "16.1473024\n",
      "Iter: 1 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 3m 56s \n",
      "Train Loss: 3.26633611437201 \n",
      "\n",
      "Test set loss: 6.842956174861299\n",
      "> tom is madly in love with mary .\n",
      "= tom esta locamente enamorado de mary .\n",
      "< tom es en en con mary con mary . <EOS>\n",
      "\n",
      "> such painters as picasso are rare .\n",
      "= existen escasos pintores como picasso .\n",
      "< un poco de muy muy muy . <EOS>\n",
      "\n",
      "> you can t use that .\n",
      "= no puedes usarlo .\n",
      "< no puedes lo que no puede eso . <EOS>\n",
      "\n",
      "> she got married when she was twenty five .\n",
      "= se caso cuando tenia veinticinco .\n",
      "< ella le le le le le le le le le le le gusta estaba . <EOS>\n",
      "\n",
      "> what were we talking about ?\n",
      "= de que estabamos hablando ?\n",
      "< que se se te que te te te te que te te te lo que te te te lo que te te lo que te te te lo que te te te que te te te que te te te te lo que te te te lo que te te te que te te te lo que te te te te lo que te te te que te te te lo que te te te que te te te lo que te te te que te te te que te te te te lo que te te lo que te\n",
      "\n",
      "> i need to speak to you .\n",
      "= necesito hablar contigo .\n",
      "< necesito que te te te puedo . <EOS>\n",
      "\n",
      "> you were with tom tonight weren t you ?\n",
      "= anoche estuviste con tom verdad ?\n",
      "< te le le le le le te que tom no te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te\n",
      "\n",
      "> i always try to be punctual .\n",
      "= intento siempre ser puntual .\n",
      "< me gusta gusta que gusta el . <EOS>\n",
      "\n",
      "> your new hair style makes you look older .\n",
      "= tu nuevo peinado te hace ver mas viejo .\n",
      "< su su favor su libro a tu ella . <EOS>\n",
      "\n",
      "> you are impossible .\n",
      "= eres imposible !\n",
      "< eres son son . <EOS>\n",
      "\n",
      "Iter: 2 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 7m 57s \n",
      "Train Loss: 2.410158142556691 \n",
      "\n",
      "Test set loss: 4.704185650392116\n",
      "> she is not a nurse but a doctor .\n",
      "= ella no es enfermera sino medico .\n",
      "< ella no es un buen que un hombre . <EOS>\n",
      "\n",
      "> you won t have that problem .\n",
      "= no tendra ese problema .\n",
      "< no lo que no se problema . <EOS>\n",
      "\n",
      "> my father left me some property .\n",
      "= mi padre me dejo algunos bienes .\n",
      "< mi padre me me ha algo de ahi . <EOS>\n",
      "\n",
      "> make an appointment .\n",
      "= pida cita .\n",
      "< un hombre una hombre . <EOS>\n",
      "\n",
      "> are they going to beat me up ?\n",
      "= van ellos a darme una paliza ?\n",
      "< estan me ellos a la escuela ? <EOS>\n",
      "\n",
      "> china is rich in natural resources .\n",
      "= china es rica en recursos naturales .\n",
      "< la casa esta en la fiesta . <EOS>\n",
      "\n",
      "> please stop singing that song .\n",
      "= deja de cantar esa cancion por favor .\n",
      "< por favor de favor que eso es . <EOS>\n",
      "\n",
      "> tom chopped firewood all afternoon .\n",
      "= tom talo lena toda la tarde .\n",
      "< tom se nos nos nos ninos . <EOS>\n",
      "\n",
      "> we live in the age of technology .\n",
      "= vivimos en la era de la tecnologia .\n",
      "< nos vivir en la escuela de la vida . <EOS>\n",
      "\n",
      "> this is tom .\n",
      "= este es tom .\n",
      "< esto es tom . <EOS>\n",
      "\n",
      "Iter: 3 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 11m 54s \n",
      "Train Loss: 2.020929272373868 \n",
      "\n",
      "Test set loss: 4.109644619259396\n",
      "> the boy splashed about in the tub .\n",
      "= el chico chapoteaba en la banera .\n",
      "< el nino se puso en la sala . <EOS>\n",
      "\n",
      "> she found a need and she filled it .\n",
      "= ella encontro una necesidad y la satisfizo .\n",
      "< ella se hizo un poco y ella . <EOS>\n",
      "\n",
      "> i can t help loving her .\n",
      "= no puedo evitar quererla .\n",
      "< no puedo ayudar a ella . <EOS>\n",
      "\n",
      "> how can you be so sure ?\n",
      "= como puedes estar tan seguro ?\n",
      "< como puede ser tan tan seguro ? <EOS>\n",
      "\n",
      "> it s an old custom .\n",
      "= es una vieja costumbre .\n",
      "< es un gran interesante . <EOS>\n",
      "\n",
      "> our teacher looks young for her age .\n",
      "= nuestra profesora se ve joven para su edad .\n",
      "< nuestro gran parece joven para su edad . <EOS>\n",
      "\n",
      "> i am almost ready .\n",
      "= estoy casi lista .\n",
      "< estoy estoy casi casi . <EOS>\n",
      "\n",
      "> the train has arrived .\n",
      "= el tren ha llegado .\n",
      "< la tren se ha musica . <EOS>\n",
      "\n",
      "> the school is two kilometers ahead .\n",
      "= el colegio esta a dos kilometros hacia adelante .\n",
      "< la escuela esta dos dos dos . <EOS>\n",
      "\n",
      "> i was doing some shopping downtown .\n",
      "= estaba haciendo algunas compras en el centro .\n",
      "< estaba haciendo haciendo de amigos de menudo . <EOS>\n",
      "\n",
      "Iter: 4 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 15m 55s \n",
      "Train Loss: 1.7699379465602392 \n",
      "\n",
      "Test set loss: 3.8691604105369874\n",
      "> she taught us singing .\n",
      "= ella nos enseno a cantar .\n",
      "< ella nos nos puso a cantar . <EOS>\n",
      "\n",
      "> it was a pretty normal party .\n",
      "= fue una fiesta bastante normal .\n",
      "< fue una mujer de la fiesta . <EOS>\n",
      "\n",
      "> he got married three days ago .\n",
      "= contrajo matrimonio hace tres dias .\n",
      "< se se quedo tres tres dias . <EOS>\n",
      "\n",
      "> tom must be furious with mary .\n",
      "= tom debe estar furioso con mary .\n",
      "< tom debe ser con mary . <EOS>\n",
      "\n",
      "> i had to go to america .\n",
      "= tuve que ir a america .\n",
      "< tenia que ir a las manos . <EOS>\n",
      "\n",
      "> i slept through the entire movie .\n",
      "= me dormi durante toda la pelicula .\n",
      "< he visto por la semana vez . <EOS>\n",
      "\n",
      "> do you feel like eating ?\n",
      "= tienes ganas de comer ?\n",
      "< te ves ganas ? <EOS>\n",
      "\n",
      "> i m getting a big raise .\n",
      "= me van a dar un gran aumento .\n",
      "< estoy esperando una gran oportunidad . <EOS>\n",
      "\n",
      "> tom is the defendant .\n",
      "= tom es el acusado .\n",
      "< tom es la mesa . <EOS>\n",
      "\n",
      "> i can t give up smoking .\n",
      "= no puedo dejar de fumar .\n",
      "< no puedo dejar de fumar . <EOS>\n",
      "\n",
      "Iter: 5 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 20m 58s \n",
      "Train Loss: 1.5861967460935804 \n",
      "\n",
      "Test set loss: 3.531958011662331\n",
      "> can i try on this jacket ?\n",
      "= me puedo probar esta chaqueta ?\n",
      "< puedo tomar con esta taza ? <EOS>\n",
      "\n",
      "> it s time to go now .\n",
      "= es hora de partir .\n",
      "< es hora de ir ahora . <EOS>\n",
      "\n",
      "> the children are gone .\n",
      "= los ninos se han ido .\n",
      "< los ninos se estan ido . <EOS>\n",
      "\n",
      "> anyone can make a mistake .\n",
      "= cualquiera puede cometer un error .\n",
      "< nadie puede hacer una error . <EOS>\n",
      "\n",
      "> she drives me mad .\n",
      "= ella me vuelve loco .\n",
      "< ella me dio mal . <EOS>\n",
      "\n",
      "> he s skinny .\n",
      "= el esta delgado .\n",
      "< el es orgulloso . <EOS>\n",
      "\n",
      "> i had a strange experience last night .\n",
      "= anoche tuve una extrana experiencia .\n",
      "< anoche tuve una persona buena anoche . <EOS>\n",
      "\n",
      "> you re really beautiful .\n",
      "= eres hermosisima .\n",
      "< eres realmente bastante . <EOS>\n",
      "\n",
      "> a really bad thing happened to him .\n",
      "= le paso algo realmente malo .\n",
      "< una verdad realmente lo paso . <EOS>\n",
      "\n",
      "> thank you for remembering my birthday .\n",
      "= gracias por recordar mi cumpleanos .\n",
      "< gracias por mi cumpleanos . <EOS>\n",
      "\n",
      "Iter: 6 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 28m 54s \n",
      "Train Loss: 1.4472829969284828 \n",
      "\n",
      "Test set loss: 3.5947017689861323\n",
      "> put the book back in the bookcase .\n",
      "= pon el libro de vuelta en el librero .\n",
      "< pon el libro en la fiesta . <EOS>\n",
      "\n",
      "> i can t pretend to like him .\n",
      "= no puedo fingir que me gusta .\n",
      "< no puedo encontrar para la gusta . <EOS>\n",
      "\n",
      "> the man died a few hours ago .\n",
      "= el hombre murio hace unas horas .\n",
      "< el hombre murio hace unos pocos horas . <EOS>\n",
      "\n",
      "> he is too shy to talk to girls .\n",
      "= es demasiado timido para hablarle a las ninas .\n",
      "< es demasiado dificil para hablar con la casa . <EOS>\n",
      "\n",
      "> these are two nice pictures .\n",
      "= estas son dos lindas fotos .\n",
      "< estos son dos flores de esos . <EOS>\n",
      "\n",
      "> tom made that mistake on purpose .\n",
      "= tom cometio ese error a proposito .\n",
      "< tom hizo que la error en la iglesia . <EOS>\n",
      "\n",
      "> i do not have any money .\n",
      "= no tengo un centimo .\n",
      "< no tengo dinero . <EOS>\n",
      "\n",
      "> tom is learning programming .\n",
      "= tomas esta aprendiendo a programar .\n",
      "< tom esta aprendiendo a mary . <EOS>\n",
      "\n",
      "> who found my wallet ?\n",
      "= quien encontro mi billetera ?\n",
      "< quien me encontro mi billetera ? <EOS>\n",
      "\n",
      "> do as he tells you .\n",
      "= hazlo como te dice .\n",
      "< haz como te lo que te dijiste . <EOS>\n",
      "\n",
      "Iter: 7 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 36m 20s \n",
      "Train Loss: 1.33272680726538 \n",
      "\n",
      "Test set loss: 3.504661610414762\n",
      "> i ll pay for this .\n",
      "= pagare por esto .\n",
      "< lo voy a pagar por esto . <EOS>\n",
      "\n",
      "> tom stashed some stuff at my place .\n",
      "= tom guardo algunas cosas en mi casa .\n",
      "< tom se hizo algo de cosas en mi lugar . <EOS>\n",
      "\n",
      "> tom did it by himself .\n",
      "= tom lo hizo por si solo .\n",
      "< tom lo hizo solo . <EOS>\n",
      "\n",
      "> he s a reckless young fellow .\n",
      "= es un muchacho muy alocado .\n",
      "< el es un buen equipo de la chica . <EOS>\n",
      "\n",
      "> there is a cup on the table .\n",
      "= hay una taza sobre la mesa .\n",
      "< hay una taza sobre la mesa . <EOS>\n",
      "\n",
      "> the lady moved here a month ago .\n",
      "= la dama se mudo aqui hace un mes .\n",
      "< la tienda se quedo aqui hace un mes . <EOS>\n",
      "\n",
      "> would you please call him back later ?\n",
      "= lo podria llamar de vuelta mas tarde ?\n",
      "< podrias llamar mas tarde ? <EOS>\n",
      "\n",
      "> keep this .\n",
      "= guarda esto .\n",
      "< no te comio esto . <EOS>\n",
      "\n",
      "> he s lived there all his life .\n",
      "= el vivio alli durante toda su vida .\n",
      "< el esta vivido alli toda su vida . <EOS>\n",
      "\n",
      "> just give me a moment .\n",
      "= dame solo un momento .\n",
      "< solo dame un momento . <EOS>\n",
      "\n",
      "Iter: 8 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 44m 25s \n",
      "Train Loss: 1.2391663250328875 \n",
      "\n",
      "Test set loss: 3.4179395347862886\n",
      "> my grandson is still a baby .\n",
      "= mi nieto es todavia un bebe .\n",
      "< mi hermana todavia tiene un bebe . <EOS>\n",
      "\n",
      "> we re both rich .\n",
      "= los dos somos ricos .\n",
      "< somos dos rico . <EOS>\n",
      "\n",
      "> i m feeding the goldfish .\n",
      "= le doy de comer al pez dorado .\n",
      "< estoy haciendo el partido de que estaba . <EOS>\n",
      "\n",
      "> please let me know what you want .\n",
      "= por favor dime lo que quieres .\n",
      "< por favor dejame lo que quieres . <EOS>\n",
      "\n",
      "> you don t need to call me .\n",
      "= no necesitas telefonearme .\n",
      "< no necesitas llamar de llamar . <EOS>\n",
      "\n",
      "> i knew tom could do it .\n",
      "= sabia que tom podria hacerlo .\n",
      "< sabia que tom lo podria hacerlo . <EOS>\n",
      "\n",
      "> what are you here for ?\n",
      "= para que estas aqui ?\n",
      "< que estas aqui ? <EOS>\n",
      "\n",
      "> don t laugh at a person in trouble .\n",
      "= no te burles de una persona en problemas .\n",
      "< no te toques en una persona en problemas . <EOS>\n",
      "\n",
      "> i d like to see some proof .\n",
      "= me gustaria ver alguna prueba .\n",
      "< me gustaria ver algo de poco . <EOS>\n",
      "\n",
      "> you should stay away from him .\n",
      "= deberias alejarte de el .\n",
      "< deberias quedarte de el . <EOS>\n",
      "\n",
      "Iter: 9 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 52m 9s \n",
      "Train Loss: 1.1619532848675318 \n",
      "\n",
      "Test set loss: 3.326292634342036\n",
      "> what time will you get to the station ?\n",
      "= a que hora llegaras a la estacion ?\n",
      "< a que hora te vas a la estacion ? <EOS>\n",
      "\n",
      "> it so happens that today is my birthday .\n",
      "= resulta que hoy es mi cumpleanos .\n",
      "< eso es tan vez que hoy es mi cumpleanos . <EOS>\n",
      "\n",
      "> he could get no more money .\n",
      "= el no podia conseguir mas dinero .\n",
      "< el no pudo un poco mas dinero . <EOS>\n",
      "\n",
      "> where did you grow up ?\n",
      "= en donde creciste ?\n",
      "< donde te has vuelto ? <EOS>\n",
      "\n",
      "> i don t want to do it again .\n",
      "= no quiero hacerlo otra vez .\n",
      "< no quiero hacerlo de nuevo . <EOS>\n",
      "\n",
      "> let s not use our real names .\n",
      "= no usemos nuestros verdaderos nombres .\n",
      "< no hablemos nuestra verdadero historia . <EOS>\n",
      "\n",
      "> he got us nice seats .\n",
      "= el nos consiguio buenos asientos .\n",
      "< el nos hizo un buen bonito . <EOS>\n",
      "\n",
      "> a mother s love is unconditional .\n",
      "= el amor de madre es incondicional .\n",
      "< es un amor de madre es la mala . <EOS>\n",
      "\n",
      "> we re studying now .\n",
      "= ahora estamos estudiando .\n",
      "< estamos estudiando ahora . <EOS>\n",
      "\n",
      "> i love winning .\n",
      "= adoro ganar .\n",
      "< me encanta los miedo . <EOS>\n",
      "\n",
      "Iter: 10 \n",
      "Learning Rate: 1 \n",
      "Time: 0h 59m 17s \n",
      "Train Loss: 1.091671714043585 \n",
      "\n",
      "Test set loss: 3.443244767025505\n",
      "> tomorrow he will land on the moon .\n",
      "= manana alunizara .\n",
      "< manana se va a los detalles . <EOS>\n",
      "\n",
      "> he made me out to be a liar .\n",
      "= me hizo quedar como un mentiroso .\n",
      "< el me hizo ser una mentiroso . <EOS>\n",
      "\n",
      "> i m quite tired .\n",
      "= estoy bastante cansado .\n",
      "< estoy bastante cansado . <EOS>\n",
      "\n",
      "> could you tell me something about yourself ?\n",
      "= puedes contarme algo acerca de ti ?\n",
      "< podrias decirme algo de ti ? <EOS>\n",
      "\n",
      "> this is all i want to do .\n",
      "= esto es todo lo que quiero hacer .\n",
      "< esto es todo que quiero hacer . <EOS>\n",
      "\n",
      "> she s good at tennis .\n",
      "= ella es buena para jugar tenis .\n",
      "< ella es bueno para el tenis . <EOS>\n",
      "\n",
      "> who is this girl ?\n",
      "= quien es esa chica ?\n",
      "< quien es esta chica ? <EOS>\n",
      "\n",
      "> i m running behind schedule .\n",
      "= voy con retraso sobre lo previsto .\n",
      "< estoy corriendo de fruta . <EOS>\n",
      "\n",
      "> i have some numbness in my left hand .\n",
      "= tengo algo dormida la mano izquierda .\n",
      "< tengo algunos de cabeza en la mano . <EOS>\n",
      "\n",
      "> he mistook me for my mother .\n",
      "= el me confundio con mi mama .\n",
      "< el me tomo a mi madre . <EOS>\n",
      "\n",
      "Iter: 11 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 6m 56s \n",
      "Train Loss: 1.0313482647546688 \n",
      "\n",
      "Test set loss: 3.320316131250917\n",
      "> there is nothing to fear .\n",
      "= no hay nada que temer .\n",
      "< no hay nada que perder . <EOS>\n",
      "\n",
      "> the crowd is still yelling .\n",
      "= la multitud sigue gritando .\n",
      "< la multitud esta en la carcel . <EOS>\n",
      "\n",
      "> keep practicing .\n",
      "= sigue practicando .\n",
      "< sigue en inglaterra . <EOS>\n",
      "\n",
      "> tom got mary drunk .\n",
      "= tom emborracho a mary .\n",
      "< tom se caso a mary . <EOS>\n",
      "\n",
      "> i can tell you what i know .\n",
      "= puedo contarle a usted lo que se .\n",
      "< te puedo decir lo que se . <EOS>\n",
      "\n",
      "> get the book .\n",
      "= consigue el libro .\n",
      "< enciende el libro . <EOS>\n",
      "\n",
      "> why don t you just shut up ?\n",
      "= por que no solo te callas ?\n",
      "< por que no te acuerdas de terminar ? <EOS>\n",
      "\n",
      "> he knocked at the door .\n",
      "= el golpeo a la puerta .\n",
      "< el golpeo a la puerta . <EOS>\n",
      "\n",
      "> let s finish what we started .\n",
      "= terminemos lo que empezamos .\n",
      "< esperemos que nosotros nosotros . <EOS>\n",
      "\n",
      "> she has a tender heart .\n",
      "= ella tiene un corazon blando .\n",
      "< ella tiene un corazon de robo . <EOS>\n",
      "\n",
      "Iter: 12 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 14m 40s \n",
      "Train Loss: 0.9775217235146763 \n",
      "\n",
      "Test set loss: 3.2526455346215304\n",
      "> if you d listen you d understand .\n",
      "= si escucharas entenderias .\n",
      "< si te gustaria ser entender . <EOS>\n",
      "\n",
      "> please hurry . we re late already .\n",
      "= haga el favor de apresurarse ya llegamos tarde .\n",
      "< por favor espere ya vamos a comprar . <EOS>\n",
      "\n",
      "> playing cards is fun .\n",
      "= jugar a las cartas es divertido .\n",
      "< jugar a las cartas son divertido . <EOS>\n",
      "\n",
      "> do you remember ?\n",
      "= lo recuerdas ?\n",
      "< recuerdas ? <EOS>\n",
      "\n",
      "> i got her a doll .\n",
      "= le traje una muneca .\n",
      "< le he encontrado una muneca . <EOS>\n",
      "\n",
      "> you shouldn t be here .\n",
      "= no deberias estar aqui .\n",
      "< no deberias estar aqui . <EOS>\n",
      "\n",
      "> your english is perfect .\n",
      "= tu ingles es perfecto .\n",
      "< tu ingles es perfecto . <EOS>\n",
      "\n",
      "> is tom really a canadian ?\n",
      "= de verdad es tom canadiense ?\n",
      "< es tom realmente canadiense ? <EOS>\n",
      "\n",
      "> give me a few .\n",
      "= dame algunos .\n",
      "< un par de unas . <EOS>\n",
      "\n",
      "> the big dog frightened the baby .\n",
      "= el gran perro asusto al bebe .\n",
      "< el gran perro le gusta al bebe . <EOS>\n",
      "\n",
      "Iter: 13 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 22m 22s \n",
      "Train Loss: 0.9286822660709155 \n",
      "\n",
      "Test set loss: 3.2857229882867656\n",
      "> he went abroad .\n",
      "= se fue al extranjero .\n",
      "< se fue al extranjero . <EOS>\n",
      "\n",
      "> their wedding is tomorrow .\n",
      "= su boda es manana .\n",
      "< su boda es manana . <EOS>\n",
      "\n",
      "> do you like oranges ?\n",
      "= os gustan las naranjas ?\n",
      "< te gustan las naranjas ? <EOS>\n",
      "\n",
      "> go two blocks and turn left .\n",
      "= avanza dos cuadras y gira a la izquierda .\n",
      "< ve a dos y la sal se se llama . <EOS>\n",
      "\n",
      "> i bought the girl a sweater .\n",
      "= le compre un sueter a la nina .\n",
      "< compre una sueter una sueter . <EOS>\n",
      "\n",
      "> there is no link between these two .\n",
      "= no hay relacion entre esos dos .\n",
      "< no hay seguro de entre estos dos . <EOS>\n",
      "\n",
      "> that woman blocked my way .\n",
      "= esa mujer se interpuso en mi camino .\n",
      "< esa mujer me puso mi manera . <EOS>\n",
      "\n",
      "> we re disappointed with the results .\n",
      "= estamos decepcionados con los resultados .\n",
      "< estamos completamente con los resultados . <EOS>\n",
      "\n",
      "> we re not supposed to be here .\n",
      "= se supone que no debemos estar aqui .\n",
      "< no estamos venido aqui . <EOS>\n",
      "\n",
      "> it was warm so i opened the window .\n",
      "= hacia calor asi es que abri la ventana .\n",
      "< estaba calor tan sabia por la ventana . <EOS>\n",
      "\n",
      "Iter: 14 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 29m 31s \n",
      "Train Loss: 0.8839674370903943 \n",
      "\n",
      "Test set loss: 3.2566601579431786\n",
      "> i remember what she said .\n",
      "= recuerdo lo que dijo .\n",
      "< recuerdo lo que dijo . <EOS>\n",
      "\n",
      "> the street lights went on .\n",
      "= se encendieron los faroles .\n",
      "< la calle se pusieron fueron . <EOS>\n",
      "\n",
      "> it s exactly what i wanted .\n",
      "= es justo lo que queria .\n",
      "< es exactamente lo que yo queria . <EOS>\n",
      "\n",
      "> i couldn t make myself understood .\n",
      "= yo no pude darme a entender .\n",
      "< no pude hacerme entender . <EOS>\n",
      "\n",
      "> it s our destiny .\n",
      "= es nuestro destino .\n",
      "< es nuestro destino . <EOS>\n",
      "\n",
      "> where are your parents ?\n",
      "= donde estan vuestros padres ?\n",
      "< donde estan tus padres ? <EOS>\n",
      "\n",
      "> i know tom is allergic to peanuts .\n",
      "= se que tom es alergico a los cacahuetes .\n",
      "< se que tom es alergico a los cacahuates . <EOS>\n",
      "\n",
      "> little remains to be done .\n",
      "= queda poco por hacer .\n",
      "< se trata a poco se ve . <EOS>\n",
      "\n",
      "> we suddenly realized what was happening .\n",
      "= repentinamente comprendimos lo que estaba pasando .\n",
      "< nos dejo de que estaba haciendo pasando . <EOS>\n",
      "\n",
      "> has she ever fallen in love ?\n",
      "= alguna vez se ha enamorado ?\n",
      "< ha vez ha llegado el amor de amor ? <EOS>\n",
      "\n",
      "Iter: 15 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 37m 41s \n",
      "Train Loss: 0.8426768902088417 \n",
      "\n",
      "Test set loss: 3.2773543172926365\n",
      "> that s so perfect .\n",
      "= es perfecto .\n",
      "< eso es tan perfecto . <EOS>\n",
      "\n",
      "> your new hair style makes you look older .\n",
      "= tu nuevo peinado te hace ver mas viejo .\n",
      "< tu nueva queda la linea te hace mucho . <EOS>\n",
      "\n",
      "> my father is becoming gray .\n",
      "= mi padre se esta quedando canoso .\n",
      "< mi padre se esta haciendo borracho . <EOS>\n",
      "\n",
      "> it is their only choice .\n",
      "= es su unica eleccion .\n",
      "< es su unica opcion . <EOS>\n",
      "\n",
      "> i assumed tom was waiting for mary .\n",
      "= asumi que tom estaba esperando a maria .\n",
      "< le di que tom estaba esperando a mary . <EOS>\n",
      "\n",
      "> davis did not want civil war .\n",
      "= davis no queria la guerra civil .\n",
      "< al realidad no queria la guerra de chicago . <EOS>\n",
      "\n",
      "> don t look back .\n",
      "= no mires atras .\n",
      "< no te preocupes . <EOS>\n",
      "\n",
      "> i don t want anyone to touch this .\n",
      "= no quiero que nadie toque esto .\n",
      "< no quiero que nadie a tocar esto . <EOS>\n",
      "\n",
      "> he saw the girl .\n",
      "= el vio a la muchacha .\n",
      "< el vio la nina . <EOS>\n",
      "\n",
      "> tom wasn t the only victim .\n",
      "= tom no fue la unica victima .\n",
      "< tom no era el unico unico . <EOS>\n",
      "\n",
      "Iter: 16 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 45m 40s \n",
      "Train Loss: 0.8060644950208863 \n",
      "\n",
      "Test set loss: 3.1840525602000174\n",
      "> it doesn t have any significance .\n",
      "= no significa nada .\n",
      "< no tiene ningun asunto . <EOS>\n",
      "\n",
      "> tom doesn t eat red meat .\n",
      "= tom no come carne roja .\n",
      "< tom no come rojo carne . <EOS>\n",
      "\n",
      "> i can be fair .\n",
      "= yo puedo ser justo .\n",
      "< puedo estar justo . <EOS>\n",
      "\n",
      "> those hens lay eggs almost every day .\n",
      "= esas gallinas ponen huevos casi todos los dias .\n",
      "< esos mujeres huevos huevos casi todos los dias . <EOS>\n",
      "\n",
      "> did you really not know that ?\n",
      "= en serio no sabias eso ?\n",
      "< de verdad no sabias eso ? <EOS>\n",
      "\n",
      "> i have horrible heartburn .\n",
      "= tengo una acidez terrible .\n",
      "< tengo un examen de medio . <EOS>\n",
      "\n",
      "> he still writes poems .\n",
      "= el todavia escribe poemas .\n",
      "< el todavia escribe poemas . <EOS>\n",
      "\n",
      "> tom and mary are on their honeymoon .\n",
      "= tom y mary estan de luna de miel .\n",
      "< tom y mary estan de la sala . <EOS>\n",
      "\n",
      "> he tidied up his room .\n",
      "= el arreglo su cuarto .\n",
      "< el se levanto su cuarto . <EOS>\n",
      "\n",
      "> is your name tom ?\n",
      "= te llamas tom ?\n",
      "< es tu nombre tom ? <EOS>\n",
      "\n",
      "Iter: 17 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 50m 20s \n",
      "Train Loss: 0.7704862288819688 \n",
      "\n",
      "Test set loss: 3.2087775433177987\n",
      "> they must work hours a day .\n",
      "= ellos deben trabajar ocho horas al dia .\n",
      "< deben trabajar horas al dia . <EOS>\n",
      "\n",
      "> i saw a friend .\n",
      "= vi a un amigo .\n",
      "< lo vi a un amigo . <EOS>\n",
      "\n",
      "> is he speaking english french or german ?\n",
      "= esta hablando en ingles frances o aleman ?\n",
      "< esta hablando ingles o aleman o aleman ? <EOS>\n",
      "\n",
      "> did you speak ?\n",
      "= hablaste ?\n",
      "< lo has hecho usted ? <EOS>\n",
      "\n",
      "> he scolded her .\n",
      "= la increpo .\n",
      "< la llamo . <EOS>\n",
      "\n",
      "> are you chinese ?\n",
      "= sois chinos ?\n",
      "< es chino ? <EOS>\n",
      "\n",
      "> i overslept again .\n",
      "= me volvi a quedar dormido .\n",
      "< me he oido otra vez . <EOS>\n",
      "\n",
      "> i didn t speak with tom .\n",
      "= no hable con tom .\n",
      "< no hablo con tom . <EOS>\n",
      "\n",
      "> it s gone out of style .\n",
      "= se ha pasado de moda .\n",
      "< ya se ha ido por la medianoche . <EOS>\n",
      "\n",
      "> water is a liquid .\n",
      "= el agua es un liquido .\n",
      "< el agua es un cerdo . <EOS>\n",
      "\n",
      "Iter: 18 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 52m 59s \n",
      "Train Loss: 0.7395090785288534 \n",
      "\n",
      "Test set loss: 3.2619364259010815\n",
      "> this is taking forever .\n",
      "= esto esta llevando una eternidad .\n",
      "< esto esta haciendo para siempre . <EOS>\n",
      "\n",
      "> her house is near the park .\n",
      "= su casa esta cerca del parque .\n",
      "< su casa esta cerca del parque . <EOS>\n",
      "\n",
      "> all the stones have been moved .\n",
      "= todas las piedras fueron movidas .\n",
      "< todas las empleados se han sido . <EOS>\n",
      "\n",
      "> i knew you d find it .\n",
      "= sabia que lo encontrarias .\n",
      "< sabia que lo que te lo explique . <EOS>\n",
      "\n",
      "> you re my enemy .\n",
      "= ustedes son mis enemigos .\n",
      "< eres mi enemigo . <EOS>\n",
      "\n",
      "> he graduated from law school .\n",
      "= se licencio en la facultad de derecho .\n",
      "< el esta en la escuela de la escuela . <EOS>\n",
      "\n",
      "> we re both rich .\n",
      "= los dos somos ricos .\n",
      "< somos dos ricos ricos . <EOS>\n",
      "\n",
      "> this is for you .\n",
      "= es para ti .\n",
      "< esto es para ti . <EOS>\n",
      "\n",
      "> you re very brave .\n",
      "= eres muy valiente .\n",
      "< eres muy valiente . <EOS>\n",
      "\n",
      "> i know who got hurt .\n",
      "= se quien se hizo dano .\n",
      "< se quien hizo herido . <EOS>\n",
      "\n",
      "Iter: 19 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 55m 26s \n",
      "Train Loss: 0.7062389708980961 \n",
      "\n",
      "Test set loss: 3.307751379138273\n",
      "> that woman blocked my way .\n",
      "= esa mujer se interpuso en mi camino .\n",
      "< esa mujer me pone de mi camino . <EOS>\n",
      "\n",
      "> we won the bronze medal .\n",
      "= nosotros ganamos la medalla de bronce .\n",
      "< nos dimos la medalla de la medalla . <EOS>\n",
      "\n",
      "> tom doesn t like eating fish .\n",
      "= a tom no le gusta comer pescado .\n",
      "< a tom no le gusta comer pescado . <EOS>\n",
      "\n",
      "> tom got shot in the back .\n",
      "= a tom le dispararon en la espalda .\n",
      "< tom se disparo en la espalda . <EOS>\n",
      "\n",
      "> i ll send you the link .\n",
      "= le enviare el enlace .\n",
      "< le voy a dar el enlace . <EOS>\n",
      "\n",
      "> we have to take the risk .\n",
      "= tenemos que arriesgarnos .\n",
      "< tenemos que tomar la boca . <EOS>\n",
      "\n",
      "> tom held up his right hand .\n",
      "= tom levanto su mano derecha .\n",
      "< tom levanto la mano derecho la mano . <EOS>\n",
      "\n",
      "> would you like white wine or red ?\n",
      "= prefiere vino blanco o tinto ?\n",
      "< quieres vino blanco o o rojo ? <EOS>\n",
      "\n",
      "> she has five older brothers .\n",
      "= ella tiene cinco hermanos mayores .\n",
      "< ella tiene cinco mayor mayor hermanos . <EOS>\n",
      "\n",
      "> i cut myself .\n",
      "= me corte .\n",
      "< me corte . <EOS>\n",
      "\n",
      "Iter: 20 \n",
      "Learning Rate: 1 \n",
      "Time: 1h 58m 15s \n",
      "Train Loss: 0.6767550183456844 \n",
      "\n",
      "Test set loss: 3.2692445042599125\n",
      "> is it true that tom went to boston ?\n",
      "= es verdad que tom fue a boston ?\n",
      "< es verdad que tom fue a boston ? <EOS>\n",
      "\n",
      "> we work every day but sunday .\n",
      "= trabajamos todos los dias excepto los domingos .\n",
      "< trabajamos todos los dias excepto el domingo . <EOS>\n",
      "\n",
      "> it was an extremely cruel war .\n",
      "= fue una guerra extremadamente cruel .\n",
      "< fue un asunto que no la guerra . <EOS>\n",
      "\n",
      "> she left with her friends .\n",
      "= ella salio con sus amigos .\n",
      "< ella se fue con sus amigos . <EOS>\n",
      "\n",
      "> i feel safe with you .\n",
      "= contigo me siento segura .\n",
      "< me siento segura contigo . <EOS>\n",
      "\n",
      "> tom and mary were also in the car .\n",
      "= tom y maria tambien estaban en el auto .\n",
      "< tom y mary estaban ocupados en el auto . <EOS>\n",
      "\n",
      "> this mustn t be exposed to the sun .\n",
      "= no hay que exponer esto al sol .\n",
      "< esto no debe ser peligroso al sol . <EOS>\n",
      "\n",
      "> may i sit here ?\n",
      "= puedo sentarme aqui ?\n",
      "< puedo puedo aqui a aqui ? <EOS>\n",
      "\n",
      "> pass me the salt and pepper please .\n",
      "= paseme la sal y la pimienta por favor .\n",
      "< pasame la sal y la pimienta por favor . <EOS>\n",
      "\n",
      "> the gun went off by accident .\n",
      "= el arma se disparo accidentalmente .\n",
      "< la pistola se fue a accidente de accidente . <EOS>\n",
      "\n",
      "Iter: 21 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 5m 32s \n",
      "Train Loss: 0.6460284568201479 \n",
      "\n",
      "Test set loss: 3.3150109965918517\n",
      "> i met tom in australia .\n",
      "= conoci a tom en australia .\n",
      "< conoci a tom en australia . <EOS>\n",
      "\n",
      "> what a wonderful present !\n",
      "= que hermoso obsequio !\n",
      "< que regalo tan maravilloso ! <EOS>\n",
      "\n",
      "> there s no need to hurry .\n",
      "= no necesitas apresurarte .\n",
      "< no hace falta prisa . <EOS>\n",
      "\n",
      "> tom still has feelings for mary .\n",
      "= tom aun tiene sentimientos por mary .\n",
      "< tom todavia tiene cosas por mary . <EOS>\n",
      "\n",
      "> i can t answer that question .\n",
      "= no puedo responder a esa pregunta .\n",
      "< no puedo responder a esa pregunta . <EOS>\n",
      "\n",
      "> i want to know who started this .\n",
      "= quiero saber quien inicio esto .\n",
      "< quiero saber quien empezo esto . <EOS>\n",
      "\n",
      "> there is a cup on the table .\n",
      "= hay una taza sobre la mesa .\n",
      "< hay una taza sobre la mesa . <EOS>\n",
      "\n",
      "> the car raised a cloud of dust .\n",
      "= el coche levanto una nube de polvo .\n",
      "< el auto levanto un agujero de polvo . <EOS>\n",
      "\n",
      "> are you from kyoto ?\n",
      "= eres de kioto ?\n",
      "< estas de kioto ? <EOS>\n",
      "\n",
      "> i bought many books .\n",
      "= compre muchos libros .\n",
      "< compre muchos libros . <EOS>\n",
      "\n",
      "Iter: 22 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 13m 29s \n",
      "Train Loss: 0.6195684786791797 \n",
      "\n",
      "Test set loss: 3.3157749406080845\n",
      "> you didn t deserve that .\n",
      "= no te merecias eso .\n",
      "< no te lo has sido . <EOS>\n",
      "\n",
      "> for here or to go ?\n",
      "= para servir o para llevar ?\n",
      "< por aqui o ir ? <EOS>\n",
      "\n",
      "> mary s my niece .\n",
      "= mary es mi sobrina .\n",
      "< mary es mi culpa . <EOS>\n",
      "\n",
      "> tom told me he s getting married .\n",
      "= tom me dijo que se va a casar .\n",
      "< tom me dijo que se esta casado . <EOS>\n",
      "\n",
      "> i eat here all the time .\n",
      "= siempre como aqui .\n",
      "< como aqui todo el tiempo . <EOS>\n",
      "\n",
      "> my father used to eat at this restaurant .\n",
      "= mi padre solia comer en este restaurante .\n",
      "< mi padre solia comer en este restaurante . <EOS>\n",
      "\n",
      "> they arrived at twilight .\n",
      "= llegaron al ocaso .\n",
      "< ellos llegaron a ellos . <EOS>\n",
      "\n",
      "> you re an overly optimistic girl .\n",
      "= sos una chica demasiado optimista .\n",
      "< eres una chica de nina optimista . <EOS>\n",
      "\n",
      "> you re not crazy .\n",
      "= no estas loco .\n",
      "< no estas loco . <EOS>\n",
      "\n",
      "> tom was unsure what to do .\n",
      "= tom no sabia que hacer .\n",
      "< tom estaba hablando que hacer . <EOS>\n",
      "\n",
      "Iter: 23 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 21m 7s \n",
      "Train Loss: 0.5946634170344688 \n",
      "\n",
      "Test set loss: 3.318880303803549\n",
      "> i should have listened more carefully .\n",
      "= deberia haber escuchado con mas atencion .\n",
      "< deberia haber escuchado mas con cuidado . <EOS>\n",
      "\n",
      "> i feel like going on a trip .\n",
      "= tengo ganas de ir de viaje .\n",
      "< tengo ganas de ir en un viaje . <EOS>\n",
      "\n",
      "> come on inside .\n",
      "= entra .\n",
      "< ven a hacer adentro . <EOS>\n",
      "\n",
      "> i can start tomorrow .\n",
      "= puedo empezar manana .\n",
      "< puedo empezar manana . <EOS>\n",
      "\n",
      "> not everything is black and white .\n",
      "= no todo es blanco y negro .\n",
      "< no todo es negro y blanco . <EOS>\n",
      "\n",
      "> this letter arrived while you were out .\n",
      "= esta carta llego mientras estabas fuera .\n",
      "< esta carta llego mientras estabas fuera . <EOS>\n",
      "\n",
      "> i wear white shirts on weekdays .\n",
      "= yo visto camisas blancas en dias de semana .\n",
      "< prefiero los pantalones camisas en los autos . <EOS>\n",
      "\n",
      "> that s what i told tom .\n",
      "= eso fue lo que le dije a tom .\n",
      "< eso es lo que le dije a tom . <EOS>\n",
      "\n",
      "> sorry i forgot .\n",
      "= perdon se me olvido .\n",
      "< lo siento olvide . <EOS>\n",
      "\n",
      "> he loves singing .\n",
      "= ama cantar .\n",
      "< le encanta cantar . <EOS>\n",
      "\n",
      "Iter: 24 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 28m 57s \n",
      "Train Loss: 0.5706075644927601 \n",
      "\n",
      "Test set loss: 3.387378230600434\n",
      "> the thief was apprehended this morning .\n",
      "= cogieron al ladron esta manana .\n",
      "< el ladron estaba viendo esta manana . <EOS>\n",
      "\n",
      "> it is an old manuscript .\n",
      "= es un viejo manuscrito .\n",
      "< es un viejo viejo . <EOS>\n",
      "\n",
      "> tom fell asleep .\n",
      "= tom se durmio .\n",
      "< tom se quedo dormido . <EOS>\n",
      "\n",
      "> tom seems to be asleep .\n",
      "= tom parece estar dormido .\n",
      "< tom parece estar dormido . <EOS>\n",
      "\n",
      "> you may use my car .\n",
      "= puedes coger mi coche .\n",
      "< puedes usar el coche . <EOS>\n",
      "\n",
      "> tomorrow is a holiday .\n",
      "= manana es feriado .\n",
      "< manana es una fiesta . <EOS>\n",
      "\n",
      "> didn t you lock up your car ?\n",
      "= no cerraste el coche ?\n",
      "< no te has entendido tu coche ? <EOS>\n",
      "\n",
      "> the dinner is almost ready .\n",
      "= la cena esta casi lista .\n",
      "< la cena esta casi lista . <EOS>\n",
      "\n",
      "> i ll decide what to do .\n",
      "= yo voy a decidir que hacer .\n",
      "< me voy a hacer que hacer . <EOS>\n",
      "\n",
      "> tom put the thermometer under his arm .\n",
      "= tom puso el termometro bajo su brazo .\n",
      "< tom puso el radio bajo el brazo . <EOS>\n",
      "\n",
      "Iter: 25 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 36m 58s \n",
      "Train Loss: 0.5481138464302112 \n",
      "\n",
      "Test set loss: 3.420916812727784\n",
      "> how many died ?\n",
      "= cuantos murieron ?\n",
      "< cuantos murieron ? <EOS>\n",
      "\n",
      "> which one do you prefer ?\n",
      "= cual prefieres tu ?\n",
      "< cual prefieres ? <EOS>\n",
      "\n",
      "> both of these are mine .\n",
      "= ambas son mias .\n",
      "< ambos son mias . <EOS>\n",
      "\n",
      "> he asked me whether she was coming .\n",
      "= me pregunto si ella vendria .\n",
      "< me pregunto si iba a venir . <EOS>\n",
      "\n",
      "> what are we waiting for ?\n",
      "= que estamos esperando ?\n",
      "< que estamos esperando ? <EOS>\n",
      "\n",
      "> tom owed mary money .\n",
      "= tom le debia dinero a mary .\n",
      "< tom le mordio dinero a mary . <EOS>\n",
      "\n",
      "> she is guilty of fraud .\n",
      "= ella es culpable de fraude .\n",
      "< ella es culpable de interprete . <EOS>\n",
      "\n",
      "> i ve only done this once before .\n",
      "= solo he hecho esto una vez .\n",
      "< solo he hecho esto una vez antes . <EOS>\n",
      "\n",
      "> i d better drive you home .\n",
      "= mejor te llevo hasta tu casa en coche .\n",
      "< mejor te seria de casa . <EOS>\n",
      "\n",
      "> turn off the gas .\n",
      "= cierra el gas .\n",
      "< apaga el gas . <EOS>\n",
      "\n",
      "Iter: 26 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 44m 32s \n",
      "Train Loss: 0.5256810898380654 \n",
      "\n",
      "Test set loss: 3.3764480671928436\n",
      "> to my amazement it disappeared in an instant .\n",
      "= para mi asombro desaparecio en un instante .\n",
      "< para mi lo dedico se rio en una falda . <EOS>\n",
      "\n",
      "> i have to go now .\n",
      "= ahora tengo que irme .\n",
      "< tengo que irme ahora . <EOS>\n",
      "\n",
      "> give me a white piece of paper .\n",
      "= dame una hoja en blanco .\n",
      "< dame un trozo blanco . <EOS>\n",
      "\n",
      "> the surgeon operated on the patient .\n",
      "= el cirujano opero al paciente .\n",
      "< la mirada entro en el paciente . <EOS>\n",
      "\n",
      "> i ve read both books .\n",
      "= me he leido los dos libros .\n",
      "< he leido los dos libros . <EOS>\n",
      "\n",
      "> at last the bus stopped .\n",
      "= por fin el autobus hizo una parada .\n",
      "< al fin el bus se detuvo . <EOS>\n",
      "\n",
      "> speak to the manager .\n",
      "= hable usted con el encargado .\n",
      "< hable con el gerente . <EOS>\n",
      "\n",
      "> i have no knife to cut with .\n",
      "= no tengo un cuchillo con que cortarlo .\n",
      "< no tengo ningun cuchillo en el . <EOS>\n",
      "\n",
      "> he hurt his knee when he fell .\n",
      "= al caer se hizo dano en la rodilla .\n",
      "< el se lastimo su rodilla cuando se cayo . <EOS>\n",
      "\n",
      "> they followed you here .\n",
      "= te siguieron hasta aqui .\n",
      "< te hicieron aqui . <EOS>\n",
      "\n",
      "Iter: 27 \n",
      "Learning Rate: 1 \n",
      "Time: 2h 52m 20s \n",
      "Train Loss: 0.5058121518427747 \n",
      "\n",
      "Test set loss: 3.440181411109251\n",
      "> do you have any good news ?\n",
      "= tienes alguna buena noticia ?\n",
      "< tienes algun buen noticia ? <EOS>\n",
      "\n",
      "> nothing s going on .\n",
      "= no pasa nada .\n",
      "< nada esta pasando . <EOS>\n",
      "\n",
      "> tom waited calmly .\n",
      "= tom espero con calma .\n",
      "< tom espero con el golpe . <EOS>\n",
      "\n",
      "> she advised him to become a teacher .\n",
      "= le aconsejo que se hiciera profesor .\n",
      "< ella le aconsejo que se convertirse en profesor . <EOS>\n",
      "\n",
      "> i don t want tom in this building .\n",
      "= no quiero a tom en este edificio .\n",
      "< no quiero a tom en este edificio . <EOS>\n",
      "\n",
      "> we spent the entire day in yoyogi park .\n",
      "= pasamos el dia entero en el parque yoyogi .\n",
      "< pasamos el dia entero en peligro parque . <EOS>\n",
      "\n",
      "> are you all by yourself ?\n",
      "= estais todas solas ?\n",
      "< estas solo tu solo ? <EOS>\n",
      "\n",
      "> they understood .\n",
      "= lo entendieron .\n",
      "< entendieron . <EOS>\n",
      "\n",
      "> learning french is useful .\n",
      "= aprender frances es util .\n",
      "< aprender frances es util . <EOS>\n",
      "\n",
      "> i can hardly hear him .\n",
      "= apenas puedo oirlo .\n",
      "< casi no puedo oirlo . <EOS>\n",
      "\n",
      "Iter: 28 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 0m 6s \n",
      "Train Loss: 0.4844733179859533 \n",
      "\n",
      "Test set loss: 3.4569850423956052\n",
      "> our marriage is over .\n",
      "= nuestro matrimonio se termino .\n",
      "< nuestro matrimonio se ha acabado . <EOS>\n",
      "\n",
      "> can i speak to tom ?\n",
      "= puedo hablar con tom ?\n",
      "< puedo hablar con tom ? <EOS>\n",
      "\n",
      "> i know who you re talking about .\n",
      "= se de quien estas hablando .\n",
      "< se de quien hablas . <EOS>\n",
      "\n",
      "> he was born in .\n",
      "= el nacio en .\n",
      "< el nacio en dos anos . <EOS>\n",
      "\n",
      "> don t forget about me .\n",
      "= no me olvides .\n",
      "< no te olvides de mi . <EOS>\n",
      "\n",
      "> tom didn t show up .\n",
      "= tom no aparecio .\n",
      "< tom no aparecio . <EOS>\n",
      "\n",
      "> the barbells are heavy .\n",
      "= las pesas son pesadas .\n",
      "< las rumores son pesada . <EOS>\n",
      "\n",
      "> i have a pain in my back .\n",
      "= tengo dolor de espalda .\n",
      "< tengo dolor en mi espalda . <EOS>\n",
      "\n",
      "> tom makes a difference .\n",
      "= tom marca la diferencia .\n",
      "< tom hace una diferencia . <EOS>\n",
      "\n",
      "> i wonder who that girl is .\n",
      "= me pregunto quien sera esa chica .\n",
      "< me pregunto quien es esa chica . <EOS>\n",
      "\n",
      "Iter: 29 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 7m 38s \n",
      "Train Loss: 0.46506958704630885 \n",
      "\n",
      "Test set loss: 3.47276870659139\n",
      "> isn t it black ?\n",
      "= acaso no es negro ?\n",
      "< no es negra ? <EOS>\n",
      "\n",
      "> i don t trust politicians .\n",
      "= no me fio de los politicos .\n",
      "< no confio en los politicos . <EOS>\n",
      "\n",
      "> he came home right at ten .\n",
      "= llego a casa exactamente a las diez .\n",
      "< el vino a casa justo justo las diez . <EOS>\n",
      "\n",
      "> i love you both .\n",
      "= las amo a las dos .\n",
      "< te amo a ambos . <EOS>\n",
      "\n",
      "> let me introduce you to my family .\n",
      "= deja que te presente a mi familia .\n",
      "< dejame presentarte a mi familia . <EOS>\n",
      "\n",
      "> don t let your guard down .\n",
      "= no bajes la guardia .\n",
      "< no dejes que tu lo sepas . <EOS>\n",
      "\n",
      "> don t be too dependent on others .\n",
      "= no dependas demasiado de los demas .\n",
      "< no seas demasiado castigado de los demas . <EOS>\n",
      "\n",
      "> he s a man of very good character .\n",
      "= es un hombre de muy buen caracter .\n",
      "< es un hombre muy bueno malo . <EOS>\n",
      "\n",
      "> for them it was the end .\n",
      "= para ellos era el fin .\n",
      "< por ellos que era el final . <EOS>\n",
      "\n",
      "> tom died of a drug overdose .\n",
      "= tom murio de una sobredosis de drogas .\n",
      "< tom murio de un veredicto de nervios . <EOS>\n",
      "\n",
      "Iter: 30 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 15m 27s \n",
      "Train Loss: 0.44771403628728995 \n",
      "\n",
      "Test set loss: 3.4204445679141515\n",
      "> who s your favorite poet ?\n",
      "= cual es tu poeta favorito ?\n",
      "< cual es tu personaje favorito ? <EOS>\n",
      "\n",
      "> i was overconfident .\n",
      "= fui vanidoso .\n",
      "< estaba extremadamente polvo . <EOS>\n",
      "\n",
      "> i couldn t kill tom .\n",
      "= no pude matar a tom .\n",
      "< no podia matar a tom . <EOS>\n",
      "\n",
      "> let s talk about the problem .\n",
      "= hablemos sobre el problema .\n",
      "< hablemos de ese problema . <EOS>\n",
      "\n",
      "> are you canadian ?\n",
      "= eres canadiense ?\n",
      "< estas canadiense ? <EOS>\n",
      "\n",
      "> he s your typical workaholic .\n",
      "= es el tipico adicto al trabajo .\n",
      "< el es tu trabajo favorito . <EOS>\n",
      "\n",
      "> you tore your pants .\n",
      "= te rajaste los pantalones .\n",
      "< te has vuelto a ver tus pantalones . <EOS>\n",
      "\n",
      "> he tried to learn french .\n",
      "= el trato de aprender frances .\n",
      "< el trato de aprender frances . <EOS>\n",
      "\n",
      "> have you seen tom s room ?\n",
      "= viste la pieza de tomas ?\n",
      "< has visto el cuarto de tomas ? <EOS>\n",
      "\n",
      "> i call her up every day .\n",
      "= la llamo todos los dias .\n",
      "< la llame todos los dias a la cabeza . <EOS>\n",
      "\n",
      "Iter: 31 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 22m 47s \n",
      "Train Loss: 0.42958215789020077 \n",
      "\n",
      "Test set loss: 3.481379714555047\n",
      "> tom doesn t even know mary .\n",
      "= tom ni siquiera conoce a maria .\n",
      "< tom no sabe ni mary . <EOS>\n",
      "\n",
      "> he lay down on the bed .\n",
      "= el se recosto sobre la cama .\n",
      "< se paro en la cama . <EOS>\n",
      "\n",
      "> i d like to reserve a hotel room .\n",
      "= quisiera reservar una habitacion en un hotel .\n",
      "< quisiera reservar una habitacion en hotel . <EOS>\n",
      "\n",
      "> we didn t go very far .\n",
      "= no fuimos tan lejos .\n",
      "< no fuimos muy lejos . <EOS>\n",
      "\n",
      "> oh boy that s embarrassing .\n",
      "= chico es embarazoso .\n",
      "< cientos al monton de carton que se sienten . <EOS>\n",
      "\n",
      "> i have not read all the books .\n",
      "= no he leido todos los libros .\n",
      "< no he leido todos los libros . <EOS>\n",
      "\n",
      "> it took me several hours to find it .\n",
      "= me tomo varias horas encontrarlo .\n",
      "< llevo a mi varias horas para encontrar . <EOS>\n",
      "\n",
      "> tom is banging on the front door .\n",
      "= tom esta golpeando la puerta principal .\n",
      "< tom esta sentado en la puerta principal . <EOS>\n",
      "\n",
      "> i would rather stay at home .\n",
      "= preferiria quedarme en casa .\n",
      "< preferiria quedarme en casa . <EOS>\n",
      "\n",
      "> tom betrayed us .\n",
      "= tom nos traiciono .\n",
      "< tom nos traiciono . <EOS>\n",
      "\n",
      "Iter: 32 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 30m 0s \n",
      "Train Loss: 0.41294351542098706 \n",
      "\n",
      "Test set loss: 3.5390963443933297\n",
      "> why weren t they there ?\n",
      "= por que no estaban alli ?\n",
      "< por que no estaban alli ? <EOS>\n",
      "\n",
      "> you ve got to keep fighting .\n",
      "= tienes que seguir luchando !\n",
      "< tienes que seguir en ti mismo . <EOS>\n",
      "\n",
      "> i ended up saying something stupid .\n",
      "= termine diciendo algo estupido .\n",
      "< me he terminado que decir algo estupido . <EOS>\n",
      "\n",
      "> she helped him tie his tie .\n",
      "= ella le ayudo a atarse la corbata .\n",
      "< ella le ayudo a limpiar su corbata . <EOS>\n",
      "\n",
      "> my cousins are coming in a few days .\n",
      "= mis primos vendran en unos dias .\n",
      "< mis ninas estan en unos dias . <EOS>\n",
      "\n",
      "> i listened but i heard nothing .\n",
      "= escuche pero no oi nada .\n",
      "< lo siento pero no oi nada . <EOS>\n",
      "\n",
      "> he puts ten dollars aside every week .\n",
      "= el se deja diez dolares cada semana .\n",
      "< el lleno diez diez queso cada semana . <EOS>\n",
      "\n",
      "> how long will you be staying here ?\n",
      "= cuanto tiempo va a quedarse aqui ?\n",
      "< cuanto tiempo vas a estar quedarte aqui ? <EOS>\n",
      "\n",
      "> tom couldn t get the book he wanted .\n",
      "= tom no pudo comprar el libro que queria .\n",
      "< tom no pudo conseguir el libro que queria . <EOS>\n",
      "\n",
      "> i love my daughter .\n",
      "= quiero a mi hija .\n",
      "< amo a mi hija . <EOS>\n",
      "\n",
      "Iter: 33 \n",
      "Learning Rate: 1 \n",
      "Time: 3h 37m 35s \n",
      "Train Loss: 0.39649004175537705 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"******************************************************************\n",
    "********************NO NEED TO ALTER ANYTHING BELOW******************\n",
    "******************************************************************\"\"\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "\"\"\"for plotting of the loss\"\"\"\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "output_file_name = \"testdata.%s_trim.%s_vocab.%s_directions.%s_layers.%s_hidden.%s_dropout.%s_learningrate.%s_batch.%s_epochs.%s\" % (dataset,trim,max_vocab_size,directions,layers,hidden_size,dropout,learning_rate,batch_size,epochs)\n",
    "\n",
    "if create_txt:\n",
    "\tprint_to = output_file_name+'.txt'\n",
    "\twith open(print_to, 'w+') as f:\n",
    "\t\tf.write(\"Starting Training \\n\")\n",
    "else:\n",
    "\tprint_to = None\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData(\n",
    "    input_lang_name, output_lang_name, raw_data_file_path, \n",
    "    max_vocab_size=max_vocab_size, reverse=reverse, trim=trim, \n",
    "    start_filter=start_filter, perc_train_set=perc_train_set, print_to=print_to)\n",
    "print('Train Pairs #')\n",
    "print(len(train_pairs))\n",
    "\n",
    "\n",
    "\"\"\"for gradient clipping from \n",
    "https://github.com/pytorch/examples/blob/master/word_language_model/main.py\"\"\"\n",
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "args = parser.parse_args()\n",
    "\n",
    "mem()\n",
    "\n",
    "if create_txt:\n",
    "\twith open(print_to, 'a') as f:\n",
    "\t\tf.write(\"\\nRandom Train Pair: %s \\n\\nRandom Test Pair: %s \\n\\n\" \n",
    "            % (random.choice(train_pairs),random.choice(test_pairs) \n",
    "               if test_pairs else \"None\"))\n",
    "\t\tf.write(mem())\n",
    "\n",
    "\n",
    "\"\"\"create the Encoder\"\"\"\n",
    "encoder = EncoderRNN(input_lang.vocab_size, hidden_size, layers=layers, \n",
    "                     dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "\"\"\"create the Decoder\"\"\"\n",
    "decoder = DecoderAttn(hidden_size, output_lang.vocab_size, layers=layers, \n",
    "                      dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "print('Encoder and Decoder Created')\n",
    "mem()\n",
    "\n",
    "if use_cuda:\n",
    "\tprint('Cuda being used')\n",
    "\tencoder = encoder.cuda()\n",
    "\tdecoder = decoder.cuda()\n",
    "\n",
    "print('Number of epochs: '+str(epochs))\n",
    "\n",
    "if create_txt:\n",
    "\twith open(print_to, 'a') as f:\n",
    "\t\tf.write('Encoder and Decoder Created\\n')\n",
    "\t\tf.write(mem())\n",
    "\t\tf.write(\"Number of epochs %s \\n\" % (epochs))\n",
    "\n",
    "train_and_test(epochs, test_eval_every, plot_every, learning_rate, lr_schedule, \n",
    "               train_pairs, test_pairs, input_lang, output_lang, batch_size, \n",
    "               test_batch_size, encoder, decoder, criterion, trim, save_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalizeString' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\enbar\\Documents\\AI Stuff\\Spanish_Translate\\Spanish_Translate\\Spanish_Translate.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/enbar/Documents/AI%20Stuff/Spanish_Translate/Spanish_Translate/Spanish_Translate.ipynb#ch0000023?line=0'>1</a>\u001b[0m outside_sent \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhose looking at me right now?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/enbar/Documents/AI%20Stuff/Spanish_Translate/Spanish_Translate/Spanish_Translate.ipynb#ch0000023?line=1'>2</a>\u001b[0m outside_sent \u001b[39m=\u001b[39m normalizeString(outside_sent)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/enbar/Documents/AI%20Stuff/Spanish_Translate/Spanish_Translate/Spanish_Translate.ipynb#ch0000023?line=2'>3</a>\u001b[0m evaluate(encoder, decoder, outside_sent, cutoff_length\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalizeString' is not defined"
     ]
    }
   ],
   "source": [
    "outside_sent = \"whose looking at me right now?\"\n",
    "outside_sent = normalizeString(outside_sent)\n",
    "evaluate(encoder, decoder, outside_sent, cutoff_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbd90dfcf26c7d9fcdd7b50e6d65d7807ca58db94cc7988a748d169fa115b27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('TRANS': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
